{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jr4vgWRBJL_",
        "outputId": "0dad9058-caf5-4884-8449-7afec008e8e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Cosine Similarity with Dynamic Time Warping (DTW)"
      ],
      "metadata": {
        "id": "tyUa04yxKZLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compare similarity between two texts\n",
        "def similar_text(seq1, seq2):\n",
        "    # Tokenize the paragraphs\n",
        "    seq1_tokens = word_tokenize(seq1)\n",
        "    seq2_tokens = word_tokenize(seq2)\n",
        "\n",
        "    # Combine tokens\n",
        "    all_tokens = [seq1_tokens, seq2_tokens]\n",
        "\n",
        "    # Vectorize tokens\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform([' '.join(tokens) for tokens in all_tokens])\n",
        "\n",
        "    # Compute cosine similarity between tokens\n",
        "    similarity_matrix = cosine_similarity(X)\n",
        "\n",
        "    # Find the maximum similarity score\n",
        "    max_similarity_score = similarity_matrix[0, 1]\n",
        "\n",
        "    return max_similarity_score, seq1_tokens, seq2_tokens\n",
        "\n",
        "# Function to calculate DTW distance\n",
        "def calculate_dtw_distance(seq1, seq2):\n",
        "    # Initialize DTW matrix with zeros\n",
        "    dtw_matrix = np.zeros((len(seq1) + 1, len(seq2) + 1))\n",
        "\n",
        "    # Fill the first row and column with large values\n",
        "    for i in range(1, len(seq1) + 1):\n",
        "        dtw_matrix[i, 0] = np.inf\n",
        "    for j in range(1, len(seq2) + 1):\n",
        "        dtw_matrix[0, j] = np.inf\n",
        "\n",
        "    # Fill the DTW matrix\n",
        "    for i in range(1, len(seq1) + 1):\n",
        "        for j in range(1, len(seq2) + 1):\n",
        "            cost = np.abs(seq1[i - 1] - seq2[j - 1])\n",
        "            dtw_matrix[i, j] = cost + min(dtw_matrix[i-1, j], dtw_matrix[i, j-1], dtw_matrix[i-1, j-1])\n",
        "\n",
        "    # Return the DTW distance and accumulated errors\n",
        "    return dtw_matrix[len(seq1), len(seq2)], dtw_matrix\n",
        "\n",
        "# Function to calculate similarity grade, DTW distance, and errors between texts\n",
        "def calculate_similarity_grade(seq1_tokens, seq2_tokens, word_to_number):\n",
        "    # Convert words in seq1 to numerical vectors using the provided mapping\n",
        "    vector1 = [word_to_number[word] for word in seq1_tokens]\n",
        "\n",
        "    # Calculate DTW distance between the sequences\n",
        "    dtw_distance, _ = calculate_dtw_distance(vector1, [word_to_number.get(word, 0) for word in seq2_tokens])\n",
        "\n",
        "    # Normalize the DTW distance to get a similarity grade out of ten\n",
        "    similarity_grade = 5 / (1 + dtw_distance)\n",
        "\n",
        "    return similarity_grade\n",
        "\n",
        "\n",
        "# Example texts and word-to-number mapping\n",
        "seq1 = \"الولد ذهب إلى المدرسة ودرس الرياضيات\"\n",
        "seq2 = \" الولد ذهب إلى المدرسة\"\n",
        "\n",
        "word_to_number = {\n",
        "    \"ذهب\": 1,\n",
        "    \"الولد\": 2,\n",
        "    \"إلى\": 3,\n",
        "    \"المدرسة\": 4,\n",
        "    \"ودرس\": 5,\n",
        "    \"الرياضيات\": 6,\n",
        "}\n",
        "\n",
        "# Compare similarity between texts\n",
        "similarity_score, seq1_tokens, seq2_tokens = similar_text(seq1, seq2)\n",
        "\n",
        "# Check if texts are similar\n",
        "if similarity_score > 0.5:  # Adjust threshold as needed\n",
        "    print(\"Texts are similar!\")\n",
        "\n",
        "    # Calculate similarity grade between the similar texts using DTW\n",
        "    similarity_grade = calculate_similarity_grade(seq1_tokens, seq2_tokens, word_to_number)\n",
        "    print(\"Similarity grade (out of 5):\", similarity_grade)\n",
        "else:\n",
        "    print(\"Texts are not similar.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzCYXz96EwJh",
        "outputId": "8ca478e9-9928-45a8-9f28-b75102abefe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texts are similar!\n",
            "Similarity grade (out of 10): 1.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Function to compare similarity between two texts\n",
        "def similar_text(seq1, seq2):\n",
        "    # Tokenize the paragraphs\n",
        "    seq1_tokens = word_tokenize(seq1)\n",
        "    seq2_tokens = word_tokenize(seq2)\n",
        "\n",
        "    # Combine tokens\n",
        "    all_tokens = [seq1_tokens, seq2_tokens]\n",
        "\n",
        "    # Vectorize tokens\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform([' '.join(tokens) for tokens in all_tokens])\n",
        "\n",
        "    # Compute cosine similarity between tokens\n",
        "    similarity_matrix = cosine_similarity(X)\n",
        "\n",
        "    # Find the maximum similarity score\n",
        "    max_similarity_score = similarity_matrix[0, 1]\n",
        "\n",
        "    return max_similarity_score, seq1_tokens, seq2_tokens\n",
        "\n",
        "# Function to calculate DTW distance\n",
        "def calculate_dtw_distance(seq1, seq2):\n",
        "    # Initialize DTW matrix with zeros\n",
        "    dtw_matrix = np.zeros((len(seq1) + 1, len(seq2) + 1))\n",
        "\n",
        "    # Fill the first row and column with large values\n",
        "    for i in range(1, len(seq1) + 1):\n",
        "        dtw_matrix[i, 0] = np.inf\n",
        "    for j in range(1, len(seq2) + 1):\n",
        "        dtw_matrix[0, j] = np.inf\n",
        "\n",
        "    # Fill the DTW matrix\n",
        "    for i in range(1, len(seq1) + 1):\n",
        "        for j in range(1, len(seq2) + 1):\n",
        "            cost = np.abs(seq1[i - 1] - seq2[j - 1])\n",
        "            dtw_matrix[i, j] = cost + min(dtw_matrix[i-1, j], dtw_matrix[i, j-1], dtw_matrix[i-1, j-1])\n",
        "\n",
        "    # Return the DTW distance and accumulated errors\n",
        "    return dtw_matrix[len(seq1), len(seq2)], dtw_matrix\n",
        "\n",
        "# Function to calculate similarity grade, DTW distance, and errors between texts\n",
        "def calculate_similarity_grade(seq1_tokens, seq2_tokens, word_to_number):\n",
        "    # Convert words in seq1 to numerical vectors using the provided mapping\n",
        "    vector1 = [word_to_number[word] for word in seq1_tokens]\n",
        "\n",
        "    # Convert words in seq2 using the mapping, if a word is not in the mapping, use 0\n",
        "    vector2 = [word_to_number.get(word, 0) for word in seq2_tokens]\n",
        "\n",
        "    # Calculate DTW distance between the sequences\n",
        "    dtw_distance, _ = calculate_dtw_distance(vector1, vector2)\n",
        "\n",
        "    # Normalize the DTW distance to get a similarity grade out of 5\n",
        "    similarity_grade = 5 / (1 + dtw_distance)\n",
        "\n",
        "    return similarity_grade\n",
        "\n",
        "# Example texts and word-to-number mapping\n",
        "seq1 = \"الولد ذهب إلى المدرسة ودرس الرياضيات\"\n",
        "seq2 = \"الولد ذهب إلى المدرسة ودرس الرياضيات\"\n",
        "\n",
        "word_to_number = {\n",
        "    \"ذهب\": 1,\n",
        "    \"الولد\": 2,\n",
        "    \"إلى\": 3,\n",
        "    \"المدرسة\": 4,\n",
        "    \"ودرس\": 5,\n",
        "    \"الرياضيات\": 6,\n",
        "}\n",
        "\n",
        "# Compare similarity between texts\n",
        "similarity_score, seq1_tokens, seq2_tokens = similar_text(seq1, seq2)\n",
        "\n",
        "# Check if texts are similar\n",
        "if similarity_score > 0.5:  # Adjust threshold as needed\n",
        "    print(\"Texts are similar!\")\n",
        "\n",
        "    # Calculate similarity grade between the similar texts using DTW\n",
        "    similarity_grade = calculate_similarity_grade(seq1_tokens, seq2_tokens, word_to_number)\n",
        "    print(\"Similarity grade (out of 5):\", similarity_grade)\n",
        "else:\n",
        "    print(\"Texts are not similar.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb_bb6ANCLdT",
        "outputId": "ace959d2-3b3c-414f-e3b5-f6cd5d74d53d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texts are similar!\n",
            "Similarity grade (out of 5): 5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. CountVectorizer"
      ],
      "metadata": {
        "id": "nHcG-rLUKe6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Function to compare similarity between two texts\n",
        "def similar_text(seq1, seq2):\n",
        "    # Tokenize the paragraphs\n",
        "    seq1_tokens = word_tokenize(seq1)\n",
        "    seq2_tokens = word_tokenize(seq2)\n",
        "\n",
        "    # Combine tokens\n",
        "    all_tokens = [seq1_tokens, seq2_tokens]\n",
        "\n",
        "    # Vectorize tokens\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform([' '.join(tokens) for tokens in all_tokens])\n",
        "\n",
        "    # Compute cosine similarity between tokens\n",
        "    similarity_matrix = cosine_similarity(X)\n",
        "\n",
        "    # Find the maximum similarity score\n",
        "    max_similarity_score = similarity_matrix[0, 1]\n",
        "\n",
        "    return max_similarity_score, seq1_tokens, seq2_tokens\n",
        "\n",
        "# Function to calculate DTW distance\n",
        "def calculate_dtw_distance(seq1, seq2):\n",
        "    # Initialize DTW matrix with zeros\n",
        "    dtw_matrix = np.zeros((len(seq1) + 1, len(seq2) + 1))\n",
        "\n",
        "    # Fill the first row and column with large values\n",
        "    for i in range(1, len(seq1) + 1):\n",
        "        dtw_matrix[i, 0] = np.inf\n",
        "    for j in range(1, len(seq2) + 1):\n",
        "        dtw_matrix[0, j] = np.inf\n",
        "\n",
        "    # Fill the DTW matrix\n",
        "    for i in range(1, len(seq1) + 1):\n",
        "        for j in range(1, len(seq2) + 1):\n",
        "            cost = abs(seq1[i - 1] - seq2[j - 1])  # Use absolute difference as cost\n",
        "            dtw_matrix[i, j] = cost + min(dtw_matrix[i-1, j], dtw_matrix[i, j-1], dtw_matrix[i-1, j-1])\n",
        "\n",
        "    # Return the DTW distance\n",
        "    return dtw_matrix[len(seq1), len(seq2)]\n",
        "\n",
        "# Function to calculate similarity grade between texts\n",
        "def calculate_similarity_grade(seq1_tokens, seq2_tokens):\n",
        "    # Calculate DTW distance between the sequences\n",
        "    dtw_distance = calculate_dtw_distance(seq1_tokens, seq2_tokens)\n",
        "\n",
        "    # Normalize the DTW distance to obtain similarity grade out of ten\n",
        "    similarity_grade = 5 / (1 + dtw_distance)\n",
        "\n",
        "    return similarity_grade\n",
        "\n",
        "# Example texts\n",
        "seq1 = \"الولد ذهب إلى المدرسة ودرس الرياضيات\"\n",
        "seq2 = \"ذهب الولد إلى المدرسة\"\n",
        "\n",
        "# Tokenize the sequences\n",
        "seq1_tokens = [1, 2, 3, 4]\n",
        "seq2_tokens = [2, 1, 3, 4, 6, 7]\n",
        "\n",
        "# Calculate similarity grade between texts\n",
        "similarity_grade = calculate_similarity_grade(seq1_tokens, seq2_tokens)\n",
        "\n",
        "# Output similarity grade\n",
        "print(\"Similarity grade (out of 5):\", similarity_grade)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_BhS5Ts3tJt",
        "outputId": "2f3b219f-d873-46ab-d378-231499c1bf09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity grade (out of 10): 0.625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Function to find the longest common subsequence (LCS) between two sequences\n",
        "def longest_common_subsequence(seq1, seq2):\n",
        "    # Initialize matrix to store lengths of LCSs\n",
        "    lcs_matrix = [[0] * (len(seq2) + 1) for _ in range(len(seq1) + 1)]\n",
        "\n",
        "    # Fill the matrix\n",
        "    for i in range(1, len(seq1) + 1):\n",
        "        for j in range(1, len(seq2) + 1):\n",
        "            if seq1[i - 1] == seq2[j - 1]:\n",
        "                lcs_matrix[i][j] = lcs_matrix[i - 1][j - 1] + 1\n",
        "            else:\n",
        "                lcs_matrix[i][j] = max(lcs_matrix[i - 1][j], lcs_matrix[i][j - 1])\n",
        "\n",
        "    # Traceback to find the LCS\n",
        "    lcs = []\n",
        "    i, j = len(seq1), len(seq2)\n",
        "    while i > 0 and j > 0:\n",
        "        if seq1[i - 1] == seq2[j - 1]:\n",
        "            lcs.insert(0, seq1[i - 1])\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "        elif lcs_matrix[i - 1][j] > lcs_matrix[i][j - 1]:\n",
        "            i -= 1\n",
        "        else:\n",
        "            j -= 1\n",
        "\n",
        "    return lcs\n",
        "\n",
        "# Function to compare similarity between two texts\n",
        "def similar_text(seq1, seq2):\n",
        "    # Tokenize the paragraphs\n",
        "    seq1_tokens = word_tokenize(seq1)\n",
        "    seq2_tokens = word_tokenize(seq2)\n",
        "\n",
        "    # Find the longest common subsequence\n",
        "    lcs = longest_common_subsequence(seq1_tokens, seq2_tokens)\n",
        "\n",
        "    return lcs\n",
        "\n",
        "# Function to calculate similarity grade between texts\n",
        "def calculate_similarity_grade(lcs_length):\n",
        "    # Calculate similarity grade based on the length of LCS\n",
        "    similarity_grade = 5 / (1 + lcs_length)\n",
        "\n",
        "    return similarity_grade\n",
        "\n",
        "# Example texts\n",
        "seq1 = \"ذهب الولد إلى المدرسة\"\n",
        "seq2 = \"ذهب الولد إلى المدرسة\"\n",
        "\n",
        "# Find the similar part between texts\n",
        "similar_part = similar_text(seq1, seq2)\n",
        "\n",
        "# Check if there is a similar part\n",
        "if similar_part:\n",
        "    print(\"Similar part found:\", ' '.join(similar_part))\n",
        "\n",
        "    # Calculate similarity grade for the similar part using the LCS\n",
        "    similarity_grade = calculate_similarity_grade(len(similar_part))\n",
        "    print(\"Similarity grade (out of 5):\", similarity_grade)\n",
        "else:\n",
        "    print(\"No similar part found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zl3h8cHs4d1O",
        "outputId": "f6038e1d-59aa-4c97-dec5-3fca2a451f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar part found: ذهب الولد إلى المدرسة\n",
            "Similarity grade (out of 10): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. SequenceMatcher"
      ],
      "metadata": {
        "id": "7178UzMqKkUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import SequenceMatcher\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Function to find the most similar contiguous part between two texts\n",
        "def find_most_similar_contiguous_part(seq1, seq2):\n",
        "    matcher = SequenceMatcher(None, seq1, seq2)\n",
        "    match = matcher.find_longest_match(0, len(seq1), 0, len(seq2))\n",
        "    return seq1[match.a: match.a + match.size]\n",
        "\n",
        "# Function to calculate similarity grade between texts\n",
        "def calculate_similarity_grade(similar_part, seq1, seq2):\n",
        "    # Calculate the similarity score based on the length of the similar part\n",
        "    similarity_score = len(similar_part) / max(len(seq1), len(seq2))\n",
        "\n",
        "    # Normalize the similarity score to obtain similarity grade out of ten\n",
        "    similarity_grade = similarity_score * 10\n",
        "\n",
        "    # If the similar part is the same as one of the original texts, set the similarity grade to 10\n",
        "    if ' '.join(similar_part) in [' '.join(seq1), ' '.join(seq2)]:\n",
        "        similarity_grade = 10\n",
        "\n",
        "    return similarity_grade\n",
        "\n",
        "# Example texts\n",
        "seq1 = \"ذهب الولد إلى المدرسة\"\n",
        "seq2 = \"ذهب الولد إلى المدرسة \"\n",
        "\n",
        "# Tokenize the sequences\n",
        "seq1_tokens = word_tokenize(seq1)\n",
        "seq2_tokens = word_tokenize(seq2)\n",
        "\n",
        "# Find the most similar contiguous part between texts\n",
        "similar_part = find_most_similar_contiguous_part(seq1_tokens, seq2_tokens)\n",
        "\n",
        "# Check if a similar part is found\n",
        "if similar_part:\n",
        "    print(\"Similar part found:\", ' '.join(similar_part))\n",
        "\n",
        "    # Calculate similarity grade for the similar part\n",
        "    similarity_grade = calculate_similarity_grade(similar_part, seq1_tokens, seq2_tokens)\n",
        "    print(\"Similarity grade (out of 5):\", similarity_grade)\n",
        "else:\n",
        "    print(\"No similar part found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djgk_ZwN6is3",
        "outputId": "2940bfb7-72a3-476a-ecc1-a7fb2832adb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar part found: ذهب الولد إلى المدرسة\n",
            "Similarity grade (out of 10): 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import SequenceMatcher\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Function to find the most similar contiguous part between two texts\n",
        "def find_most_similar_contiguous_part(seq1, seq2):\n",
        "    matcher = SequenceMatcher(None, seq1, seq2)\n",
        "    match = matcher.find_longest_match(0, len(seq1), 0, len(seq2))\n",
        "    return seq1[match.a: match.a + match.size]\n",
        "\n",
        "# Function to calculate similarity grade between texts\n",
        "def calculate_similarity_grade(similar_part, seq1, seq2):\n",
        "    # Calculate the similarity score based on the length of the similar part\n",
        "    similarity_score = len(similar_part) / max(len(seq1), len(seq2))\n",
        "\n",
        "    # Normalize the similarity score to obtain similarity grade out of ten\n",
        "    similarity_grade = similarity_score * 10\n",
        "\n",
        "    # If the similar part is the same as one of the original texts, set the similarity grade to 10\n",
        "    if ' '.join(similar_part) in [' '.join(seq1), ' '.join(seq2)]:\n",
        "        similarity_grade = 10\n",
        "\n",
        "    return similarity_grade\n",
        "\n",
        "# Example texts\n",
        "seq1 = \"ذهب الولد إلى المدرسة \"\n",
        "seq2 = \"الولد ذهب إلى المدرسة\"\n",
        "\n",
        "# Tokenize the sequences\n",
        "seq1_tokens = word_tokenize(seq1)\n",
        "seq2_tokens = word_tokenize(seq2)\n",
        "\n",
        "# Find the most similar contiguous part between texts\n",
        "similar_part = find_most_similar_contiguous_part(seq1_tokens, seq2_tokens)\n",
        "\n",
        "# Check if a similar part is found\n",
        "if similar_part:\n",
        "    print(\"Similar part found:\", ' '.join(similar_part))\n",
        "\n",
        "    # Calculate similarity grade for the similar part\n",
        "    similarity_grade = calculate_similarity_grade(similar_part, seq1_tokens, seq2_tokens)\n",
        "    print(\"Similarity grade (out of 5):\", similarity_grade)\n",
        "else:\n",
        "    print(\"No similar part found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MYC1YKG67Uj",
        "outputId": "5d0c5218-b4e8-492d-e8b6-1f93b1a6e06b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar part found: إلى المدرسة\n",
            "Similarity grade (out of 10): 5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. WER with Levenshtein Distances Enhanced by DTW"
      ],
      "metadata": {
        "id": "ikG5hFNpKpJG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsyflTJAw2vA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e4b6889-ffbd-48eb-8328-0d70ddb97d11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WER: 0.25\n",
            "Grade: 4\n",
            "Marked Reference: كان ياما كان في قديم $زمان$ وسالف العصر $والاون$ وكان $هنالك$ ملك عظيم يحكم بلاد\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate DTW distance\n",
        "def calculate_dtw_distance(s1, s2):\n",
        "    # Initialize DTW matrix with zeros\n",
        "    dtw_matrix = np.zeros((len(s1) + 1, len(s2) + 1))\n",
        "\n",
        "    # Fill the first row and column with large values\n",
        "    for i in range(1, len(s1) + 1):\n",
        "        dtw_matrix[i, 0] = np.inf\n",
        "    for j in range(1, len(s2) + 1):\n",
        "        dtw_matrix[0, j] = np.inf\n",
        "\n",
        "    # Fill the DTW matrix\n",
        "    for i in range(1, len(s1) + 1):\n",
        "        for j in range(1, len(s2) + 1):\n",
        "            cost = abs(ord(s1[i - 1][0]) - ord(s2[j - 1][0]))\n",
        "            dtw_matrix[i, j] = cost + min(dtw_matrix[i-1, j], dtw_matrix[i, j-1], dtw_matrix[i-1, j-1])\n",
        "\n",
        "    # Return the DTW distance and accumulated errors\n",
        "    return dtw_matrix[len(s1), len(s2)], dtw_matrix\n",
        "\n",
        "def levenshtein(s1, s2):\n",
        "    if len(s1) < len(s2):\n",
        "        s1, s2 = s2, s1\n",
        "\n",
        "    if len(s2) == 0:\n",
        "        return len(s1), []\n",
        "\n",
        "    previous_row = range(len(s2) + 1)\n",
        "    table = [previous_row]\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (c1 != c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "        table.append(current_row)\n",
        "\n",
        "    # Traceback to find the alignment and mark errors\n",
        "    marked_reference = []\n",
        "    x, y = len(s1), len(s2)\n",
        "    while x > 0 and y > 0:\n",
        "        if s1[x-1] == s2[y-1]:\n",
        "            marked_reference.append(s2[y-1])\n",
        "            x, y = x-1, y-1\n",
        "        elif table[x][y] == table[x-1][y-1] + 1:  # Substitution\n",
        "            marked_reference.append('$' + s2[y-1] + '$')\n",
        "            x, y = x-1, y-1\n",
        "        elif table[x][y] == table[x][y-1] + 1:  # Insertion in s2\n",
        "            marked_reference.append('$' + s2[y-1] + '$')\n",
        "            y -= 1\n",
        "        else:  # Deletion in s1\n",
        "            x -= 1\n",
        "\n",
        "    # Catch remaining parts of the reference that are missing in the transcript\n",
        "    while y > 0:\n",
        "        marked_reference.append('$' + s2[y-1] + '$')\n",
        "        y -= 1\n",
        "\n",
        "    marked_reference.reverse()\n",
        "    return table[-1][-1], ' '.join(marked_reference)\n",
        "\n",
        "def calculate_wer(transcript, reference):\n",
        "    transcript_words = transcript.split()\n",
        "    reference_words = reference.split()\n",
        "    _, dtw_matrix = calculate_dtw_distance(transcript_words, reference_words)\n",
        "    distance, marked_reference = levenshtein(transcript_words, reference_words)\n",
        "    wer = distance / max(len(transcript_words), len(reference_words))\n",
        "    return wer, marked_reference\n",
        "\n",
        "def grade_transcript(wer):\n",
        "    if wer < 0.1: return 5\n",
        "    if wer < 0.2: return 4.5\n",
        "    if wer < 0.3: return 4\n",
        "    if wer < 0.4: return 3.5\n",
        "    if wer < 0.5: return 3\n",
        "    if wer < 0.6: return 2.5\n",
        "    if wer < 0.7: return 2\n",
        "    if wer < 0.8: return 1.5\n",
        "    if wer < 0.9: return 1\n",
        "    return 0.5 if wer < 1 else 0\n",
        "\n",
        "# Example usage\n",
        "reference = \"كان ياما كان في قديم الزمان وسالف العصر والأوان وكان هناك ملك عظيم الشأن يحكم بلاد  \"\n",
        "transcript = \"كان ياما كان في قديم زمان وسالف العصر والاون وكان هنالك ملك عظيم يحكم بلاد \"\n",
        "\n",
        "wer, marked_reference = calculate_wer(transcript, reference)\n",
        "grade = grade_transcript(wer)\n",
        "\n",
        "print(\"WER:\", wer)\n",
        "print(\"Grade:\", grade)\n",
        "print(\"Marked Reference:\", marked_reference)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate DTW distance\n",
        "def calculate_dtw_distance(s1, s2):\n",
        "    # Initialize DTW matrix with zeros\n",
        "    dtw_matrix = np.zeros((len(s1) + 1, len(s2) + 1))\n",
        "\n",
        "    # Fill the first row and column with large values\n",
        "    for i in range(1, len(s1) + 1):\n",
        "        dtw_matrix[i, 0] = np.inf\n",
        "    for j in range(1, len(s2) + 1):\n",
        "        dtw_matrix[0, j] = np.inf\n",
        "\n",
        "    # Fill the DTW matrix\n",
        "    for i in range(1, len(s1) + 1):\n",
        "        for j in range(1, len(s2) + 1):\n",
        "            cost = abs(ord(s1[i - 1][0]) - ord(s2[j - 1][0]))\n",
        "            dtw_matrix[i, j] = cost + min(dtw_matrix[i-1, j], dtw_matrix[i, j-1], dtw_matrix[i-1, j-1])\n",
        "\n",
        "    # Return the DTW distance and accumulated errors\n",
        "    return dtw_matrix[len(s1), len(s2)], dtw_matrix\n",
        "\n",
        "def levenshtein(s1, s2):\n",
        "    if len(s1) < len(s2):\n",
        "        s1, s2 = s2, s1\n",
        "\n",
        "    if len(s2) == 0:\n",
        "        return len(s1), []\n",
        "\n",
        "    previous_row = range(len(s2) + 1)\n",
        "    table = [previous_row]\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (c1 != c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "        table.append(current_row)\n",
        "\n",
        "    # Traceback to find the alignment and mark errors\n",
        "    marked_reference = []\n",
        "    x, y = len(s1), len(s2)\n",
        "    while x > 0 and y > 0:\n",
        "        if s1[x-1] == s2[y-1]:\n",
        "            marked_reference.append(s2[y-1])\n",
        "            x, y = x-1, y-1\n",
        "        elif table[x][y] == table[x-1][y-1] + 1:  # Substitution\n",
        "            marked_reference.append('$' + s2[y-1] + '$')\n",
        "            x, y = x-1, y-1\n",
        "        elif table[x][y] == table[x][y-1] + 1:  # Insertion in s2\n",
        "            marked_reference.append('$' + s2[y-1] + '$')\n",
        "            y -= 1\n",
        "        else:  # Deletion in s1\n",
        "            x -= 1\n",
        "\n",
        "    # Catch remaining parts of the reference that are missing in the transcript\n",
        "    while y > 0:\n",
        "        marked_reference.append('$' + s2[y-1] + '$')\n",
        "        y -= 1\n",
        "\n",
        "    marked_reference.reverse()\n",
        "    return table[-1][-1], ' '.join(marked_reference)\n",
        "\n",
        "def calculate_wer(transcript, reference):\n",
        "    transcript_words = transcript.split()\n",
        "    reference_words = reference.split()\n",
        "    _, dtw_matrix = calculate_dtw_distance(transcript_words, reference_words)\n",
        "    distance, marked_reference = levenshtein(transcript_words, reference_words)\n",
        "    wer = distance / max(len(transcript_words), len(reference_words))\n",
        "    return wer, marked_reference\n",
        "\n",
        "def grade_transcript(wer):\n",
        "    if wer < 0.1: return 5\n",
        "    if wer < 0.2: return 4.5\n",
        "    if wer < 0.3: return 4\n",
        "    if wer < 0.4: return 3.5\n",
        "    if wer < 0.5: return 3\n",
        "    if wer < 0.6: return 2.5\n",
        "    if wer < 0.7: return 2\n",
        "    if wer < 0.8: return 1.5\n",
        "    if wer < 0.9: return 1\n",
        "    return 0.5 if wer < 1 else 0\n",
        "\n",
        "# Example usage\n",
        "reference = \"الثعلب البني السريع يقفز فوق الكلب الكسول\"\n",
        "transcript = \"الثعلب البني السريع يقفز فوق الكلب الكسول\"\n",
        "\n",
        "wer, marked_reference = calculate_wer(transcript, reference)\n",
        "grade = grade_transcript(wer)\n",
        "\n",
        "print(\"WER:\", wer)\n",
        "print(\"Grade:\", grade)\n",
        "print(\"Marked Reference:\", marked_reference)"
      ],
      "metadata": {
        "id": "mixITUBpxANf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5910aded-3316-44d9-cdb0-af4d1b5509d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WER: 0.0\n",
            "Grade: 5\n",
            "Marked Reference: الثعلب البني السريع يقفز فوق الكلب الكسول\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Our Grading system :**"
      ],
      "metadata": {
        "id": "bOWp7Mi8LfR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate DTW distance\n",
        "def calculate_dtw_distance(s1, s2):\n",
        "    # Create a matrix initialized with zeros\n",
        "    dtw_matrix = np.zeros((len(s1) + 1, len(s2) + 1))\n",
        "\n",
        "    # Initialize the first row and column with infinity (np.inf)\n",
        "    dtw_matrix[1:, 0] = np.inf\n",
        "    dtw_matrix[0, 1:] = np.inf\n",
        "\n",
        "    # Calculate DTW by comparing elements and considering the minimum path to each cell\n",
        "    for i in range(1, len(s1) + 1):\n",
        "        for j in range(1, len(s2) + 1):\n",
        "            cost = abs(ord(s1[i - 1][0]) - ord(s2[j - 1][0]))\n",
        "            dtw_matrix[i, j] = cost + min(dtw_matrix[i-1, j],    # Insertion\n",
        "                                          dtw_matrix[i, j-1],    # Deletion\n",
        "                                          dtw_matrix[i-1, j-1])  # Match or substitution\n",
        "\n",
        "    # Return the DTW distance and the entire matrix\n",
        "    return dtw_matrix[len(s1), len(s2)], dtw_matrix\n",
        "\n",
        "# Function to calculate Levenshtein distance\n",
        "def levenshtein(s1, s2):\n",
        "    dp = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]\n",
        "\n",
        "    # Base cases initialization\n",
        "    for i in range(len(s1) + 1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(len(s2) + 1):\n",
        "        dp[0][j] = j\n",
        "\n",
        "    # Fill the DP table\n",
        "    for i in range(1, len(s1) + 1):\n",
        "        for j in range(1, len(s2) + 1):\n",
        "            if s1[i - 1] == s2[j - 1]:\n",
        "                dp[i][j] = dp[i - 1][j - 1]\n",
        "            else:\n",
        "                dp[i][j] = 1 + min(dp[i - 1][j],    # Insertion\n",
        "                                   dp[i][j - 1],    # Deletion\n",
        "                                   dp[i - 1][j - 1])  # Substitution\n",
        "\n",
        "    # Construct marked differences for visual feedback\n",
        "    marked_reference = []\n",
        "    i, j = len(s1), len(s2)\n",
        "    while i > 0 or j > 0:\n",
        "        if i > 0 and j > 0 and s1[i-1] == s2[j-1]:\n",
        "            marked_reference.append(s1[i-1])\n",
        "            i, j = i - 1, j - 1\n",
        "        elif i > 0 and (j == 0 or dp[i][j] == dp[i-1][j] + 1):\n",
        "            marked_reference.append(f'${s1[i-1]}$')\n",
        "            i -= 1\n",
        "        else:\n",
        "            j -= 1\n",
        "\n",
        "    marked_reference.reverse()\n",
        "    return dp[len(s1)][len(s2)], ' '.join(marked_reference)\n",
        "\n",
        "# Function to calculate Word Error Rate (WER)\n",
        "def calculate_wer(transcript, reference):\n",
        "    transcript_words = transcript.split()\n",
        "    reference_words = reference.split()\n",
        "    _, dtw_matrix = calculate_dtw_distance(transcript_words, reference_words)\n",
        "    distance, marked_reference = levenshtein(reference_words, transcript_words)\n",
        "    wer = distance / max(len(transcript_words), len(reference_words))\n",
        "    return wer, marked_reference\n",
        "\n",
        "# Function to grade the transcript based on WER, scaled to a max score of 5\n",
        "def grade_transcript(wer):\n",
        "    if wer < 0.1: return 5\n",
        "    if wer < 0.2: return 4.5\n",
        "    if wer < 0.3: return 4\n",
        "    if wer < 0.4: return 3.5\n",
        "    if wer < 0.5: return 3\n",
        "    if wer < 0.6: return 2.5\n",
        "    if wer < 0.7: return 2\n",
        "    if wer < 0.8: return 1.5\n",
        "    if wer < 0.9: return 1\n",
        "    return 0.5 if wer < 1 else 0\n",
        "\n",
        "# Example usage\n",
        "reference = \"ذهب الولد إلى المدرسة ودرس الرياضيات\"\n",
        "transcript = \"ذهب الولد إلى المدرسة\"\n",
        "\n",
        "wer, marked_reference = calculate_wer(transcript, reference)\n",
        "grade = grade_transcript(wer)\n",
        "\n",
        "print(\"WER:\", wer)\n",
        "print(\"Grade:\", grade)\n",
        "print(\"Marked Reference:\", marked_reference)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klvj3nzpD6f1",
        "outputId": "11c26e0e-ce66-4da6-d2ec-339eae9629af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WER: 0.3333333333333333\n",
            "Grade: 3.5\n",
            "Marked Reference: ذهب الولد إلى المدرسة $ودرس$ $الرياضيات$\n"
          ]
        }
      ]
    }
  ]
}