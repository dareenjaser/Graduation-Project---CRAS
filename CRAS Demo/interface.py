# -*- coding: utf-8 -*-
"""Yet_Another_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nEV4X6vTJryCK0kFkjjWzEdWPaWWCtr0

### CRAS Pipeline
"""



import pandas as pd
import re
import noisereduce as nr
from IPython.display import Audio
from playsound import playsound
from gtts import gTTS
import speech_recognition as sr
from pydub import AudioSegment
import urllib.request
import soundfile as sf
import numpy as np
import urllib.request
import io
from transformers import WhisperForConditionalGeneration, WhisperProcessor
from transformers import WhisperFeatureExtractor,WhisperTokenizer
from huggingface_hub import notebook_login

import streamlit as st
from pydub import AudioSegment
from pydub.utils import mediainfo
from io import BytesIO





data = pd.read_csv('/content/PSUT_Needed_Data_-_Book_Content.csv')
book_ids = data['book id'].unique()
books_dict = {}

for book in book_ids :
  book_contents =[]
  subset = data[data['book id'] == book]
  for i in range(len(subset)) :
    slide = subset['paragraph text'].iloc[i]
    res = re.sub(r'[^\w\s#]', '', slide)
    res = re.sub(r'#', ' ', res)
    book_contents.append(res)
  joined_book_contents = " ".join(book_contents)
  books_dict[book] = joined_book_contents

books = pd.DataFrame(books_dict.values(), index=books_dict.keys())
books.columns = ['Text']


def receive_voice(url, index=0):
    try:
      try :
          response = urllib.request.urlopen(url)
          audio, rate = sf.read(io.BytesIO(response.read()))
          return audio, rate 
      except:
        audio, rate = sf.read(url)
        return audio, rate 

    except IndexError:
        print("The specified index is out of the dataset range.")
        return None, None, None
    except FileNotFoundError:
        print("The specified voice file was not found.")
        return None, None, None
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None, None , None



def fetch_reference_transcript(Book_id):
    try:
        return books['Text'].loc[Book_id]
    except KeyError:
        print(f"No transcript found for Book ID: {Book_id}")
        return None


def denoise_audio(voice_data,sample_rate):
    reduced_noise = nr.reduce_noise(y = voice_data, sr=sample_rate, n_std_thresh_stationary=1.5,stationary=False)
    path = '/content/denoised_file.mp3'
    sf.write(path, reduced_noise, sample_rate)
    return path

notebook_login()



from transformers import AutoModel

model = AutoModel.from_pretrained("dana2002/lastone", use_auth_token="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")

from transformers import pipeline
import torch
import os
device = "cuda" if torch.cuda.is_available() else "cpu"



def transcribe_audio(voice_data):

  model = WhisperForConditionalGeneration.from_pretrained("dana2002/lastone")
  feature_extractor = WhisperFeatureExtractor.from_pretrained("dana2002/finalized-tokenizer")
  tokenizer = WhisperTokenizer.from_pretrained("dana2002/finalized-tokenizer")
  processor = WhisperProcessor.from_pretrained("dana2002/finalized-tokenizer")
  model = pipeline(
    task="automatic-speech-recognition",
    model="dana2002/augm-model",
    device=device,
    chunk_length_s=30,
    generate_kwargs = {"num_beams":5},
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor
  )


  dirc = r"/content/"
  files = [os.path.join(dirc, f) for f in os.listdir(dirc)]
  i=0

  result = model(voice_data)
  print(f"Transcription for {voice_data}:")
  print(result)
  return result['text']
 

def calculate_dtw_distance(s1, s2):
    # Initialize DTW matrix with zeros
    dtw_matrix = np.zeros((len(s1) + 1, len(s2) + 1))

    # Fill the first row and column with large values
    for i in range(1, len(s1) + 1):
        dtw_matrix[i, 0] = np.inf
    for j in range(1, len(s2) + 1):
        dtw_matrix[0, j] = np.inf

    # Fill the DTW matrix
    for i in range(1, len(s1) + 1):
        for j in range(1, len(s2) + 1):
            cost = abs(ord(s1[i - 1][0]) - ord(s2[j - 1][0]))
            dtw_matrix[i, j] = cost + min(dtw_matrix[i-1, j], dtw_matrix[i, j-1], dtw_matrix[i-1, j-1])

    # Return the DTW distance and accumulated errors
    return dtw_matrix[len(s1), len(s2)], dtw_matrix

def levenshtein(s1, s2):
    if len(s1) < len(s2):
        s1, s2 = s2, s1

    if len(s2) == 0:
        return len(s1), []

    previous_row = range(len(s2) + 1)
    table = [previous_row]
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
        table.append(current_row)

    # Traceback to find the alignment and mark errors
    marked_reference = []
    x, y = len(s1), len(s2)
    while x > 0 and y > 0:
        if s1[x-1] == s2[y-1]:
            marked_reference.append(s2[y-1])
            x, y = x-1, y-1
        elif table[x][y] == table[x-1][y-1] + 1:  # Substitution
            marked_reference.append('<span style="color:red;">' + s2[y-1] + '</span>')
            x, y = x-1, y-1
        elif table[x][y] == table[x][y-1] + 1:  # Insertion in s2
            marked_reference.append('<span style="color:red;">' + s2[y-1] + '</span>')
            y -= 1
        else:  # Deletion in s1
            x -= 1

    
    while y > 0:
        marked_reference.append('<span style="color:red;">' + s2[y-1] + '</span>')
        y -= 1

    marked_reference.reverse()
    return table[-1][-1], ' '.join(marked_reference)

def calculate_wer(transcript, reference):
    transcript_words = transcript.split()
    reference_words = reference.split()
    _, dtw_matrix = calculate_dtw_distance(transcript_words, reference_words)
    distance, marked_reference = levenshtein(transcript_words, reference_words)
    wer = distance / max(len(transcript_words), len(reference_words))
    return wer, marked_reference

def grade_transcript(wer):
    if wer < 0.1: return 10
    if wer < 0.2: return 9
    if wer < 0.3: return 8
    if wer < 0.4: return 7
    if wer < 0.5: return 6
    if wer < 0.6: return 5
    if wer < 0.7: return 4
    if wer < 0.8: return 3
    if wer < 0.9: return 2
    if wer >= 1: return 1
    return 0


def calculate_wer(transcript, reference):
    transcript_words = transcript.split()
    reference_words = reference.split()
    distance, marked_reference = levenshtein(transcript_words, reference_words)
    wer = distance / max(len(transcript_words), len(reference_words))
    return grade_transcript(wer), marked_reference







def read_audio(file):
    file_format = file.type.split('/')[-1]
    audio = AudioSegment.from_file(file, format=file_format)
    info = mediainfo(file.name)
    sample_rate = int(info['sample_rate'])
    return audio, sample_rate
import streamlit as st

Books_mapping = {
  'حرف الباء' : 545,
  "الحيوانات" : 9,
  "حرف الجيم" : 548,
  "حرف السين" : 555
}

st.title("Children Reading Assesment System")



book_names = ['حرف الباء', "الحيوانات", "حرف الجيم", "حرف السين"]
selected_book = st.selectbox("Choose a book:", book_names)

book_id = Books_mapping[selected_book]
st.write(f"Selected book: {selected_book}")
audio, sample_rate = None , None
uploaded_file = st.file_uploader("Upload the audio file you want to grade:", type=["mp3", "wav", "ogg"])
if uploaded_file is not None:
    # Display the uploaded file
    
    st.audio(uploaded_file, format='audio/wav', start_time=0)
    
    

else:
    st.write("Please upload an audio file.")

if st.button("Submit"):

    voice_data, sample_rate = receive_voice(uploaded_file)
    reference_text = fetch_reference_transcript(book_id)
    cleaned_voice_data = denoise_audio(voice_data,sample_rate)
    transcribed_text = transcribe_audio(cleaned_voice_data)
    if transcribed_text == None:
      print('Error occured in transcription system')
    else :
      grading_result , marked_reference= calculate_wer( reference_text,transcribed_text)
      st.write(f"Student Grade: {grading_result}")

      st.markdown(f"**Marked Reference:** {marked_reference}", unsafe_allow_html=True)

    print( 'Error occured in transcription system' )

